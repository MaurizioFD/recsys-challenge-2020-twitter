{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load unique tweet tokens from file \n",
    "# Remove mentions and hashtags from tweets\n",
    "\n",
    "### Save in another file the number of mentions for that tweet and the mentions list (same for hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from TokenizerWrapper import TokenizerWrapper\n",
    "from TokenizerWrapper import special_tokens\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_ID = \"tweet_features_tweet_id\"\n",
    "TWEET_TOKENS = \"tweet_features_text_tokens\"\n",
    "\n",
    "TWEET_TOKENS_FILE = \"tweet_tokens/text_tokens_all_no_escaped_chars.csv\"  #\"tweet_tokens/tweet_text_longer_than_280_no_escaped_chars.csv\"\n",
    "\n",
    "RESULT_PATH = \"tweet_tokens/text_tokens_clean_2.csv\"\n",
    "MENTIONS_PATH = \"tweet_tokens/mentions/mentions.csv\"\n",
    "HASHTAGS_PATH = \"tweet_tokens/hashtags/hashtags.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to extract mentions and hashtags from the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return text_tokens, mentions_list, mentions_count\n",
    "# in case the tweet is a retweet\n",
    "def get_RT_mentions(tokens, mentions):\n",
    "\n",
    "    length = len(tokens)-1\n",
    "    \n",
    "    i = 2  # exclude CLS and the 56898 ('RT') token\n",
    "    while tokens[i] != special_tokens[':'] and i < length:\n",
    "        i += 1\n",
    "\n",
    "    #print('i: ' + str(i))\n",
    "\n",
    "    mentions.append(tokens[2:i])\n",
    "    #mentions.append('102\\n') # append SEP \\n\n",
    "\n",
    "    tokens = tokens[i+1:]\n",
    "    tokens.insert(0, '101')   # insert CLS at beginning\n",
    "    \n",
    "    return tokens, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(tokens, mentions):\n",
    "    \n",
    "    found_initial = False\n",
    "        \n",
    "    initial_index = 0\n",
    "    final_index = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        \n",
    "        t = tokens[i]\n",
    "        \n",
    "        if t == special_tokens['@'] and not found_initial:\n",
    "            initial_index = i\n",
    "            found_initial = True\n",
    "            \n",
    "        elif found_initial and i==initial_index+1:\n",
    "            pass\n",
    "        \n",
    "        elif found_initial and i > initial_index+1:\n",
    "            decoded_t = tok.convert_tokens_to_strings([t])[0]\n",
    "            if '##' in decoded_t:\n",
    "                pass\n",
    "            elif '_' == decoded_t:\n",
    "                pass\n",
    "            elif tok.convert_tokens_to_strings([tokens[i-1]])[0] == '_':\n",
    "                pass\n",
    "            else:\n",
    "                final_index = i\n",
    "                mentions.append(tokens[initial_index:final_index])\n",
    "                found_initial = False\n",
    "            \n",
    "    return tokens, mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remove_mentions(tokens, mentions):\n",
    "    \n",
    "    found_initial = False\n",
    "    \n",
    "    mask = []\n",
    "    \n",
    "    initial_index = 0\n",
    "    final_index = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        \n",
    "        t = tokens[i]\n",
    "            \n",
    "        if found_initial and i==initial_index+1:\n",
    "            mask.append(False)\n",
    "        \n",
    "        elif found_initial and i > initial_index+1:\n",
    "            decoded_t = tok.convert_tokens_to_strings([t])[0]\n",
    "            if '##' in decoded_t:\n",
    "                mask.append(False)\n",
    "            elif '_' == decoded_t:\n",
    "                mask.append(False)\n",
    "            elif tok.convert_tokens_to_strings([tokens[i-1]])[0] == '_':\n",
    "                mask.append(False)\n",
    "            else:\n",
    "                final_index = i\n",
    "                mentions.append(tokens[initial_index:final_index])\n",
    "                found_initial = False\n",
    "                # mask.append(True)\n",
    "    \n",
    "                \n",
    "        if not found_initial and t == special_tokens['@']:\n",
    "            initial_index = i\n",
    "            found_initial = True\n",
    "            mask.append(False)\n",
    "            \n",
    "        elif not found_initial:\n",
    "            mask.append(True)\n",
    "        \n",
    "            #print(decoded_t)\n",
    "    tokens_arr = np.array(tokens)\n",
    "    tokens_arr = tokens_arr[mask]\n",
    "    tokens = tokens_arr.tolist()\n",
    "    \n",
    "    return tokens, mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remove_mentions_hashtags(tokens, mentions, hashtags):\n",
    "    \n",
    "    found_initial = False\n",
    "    \n",
    "    mask = []\n",
    "    \n",
    "    initial_index = 0\n",
    "    final_index = 0\n",
    "    is_mention = False\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        \n",
    "        t = tokens[i]\n",
    "        \n",
    "        if found_initial and i==initial_index+1:\n",
    "            mask.append(False)\n",
    "        \n",
    "        elif found_initial and i > initial_index+1:\n",
    "            decoded_t = tok.convert_tokens_to_strings([t])[0]\n",
    "            if '##' in decoded_t:\n",
    "                mask.append(False)\n",
    "            elif '_' == decoded_t:\n",
    "                mask.append(False)\n",
    "            elif tok.convert_tokens_to_strings([tokens[i-1]])[0] == '_':\n",
    "                mask.append(False)\n",
    "            else:\n",
    "                final_index = i\n",
    "                if is_mention:\n",
    "                    mentions.append(tokens[initial_index:final_index])\n",
    "                else:\n",
    "                    hashtags.append(tokens[initial_index:final_index])\n",
    "\n",
    "                found_initial = False\n",
    "                # mask.append(True)\n",
    "    \n",
    "                \n",
    "        if not found_initial and (t == special_tokens['@'] or t == special_tokens['#']):\n",
    "            if t == special_tokens['@']:\n",
    "                is_mention = True\n",
    "            elif t == special_tokens['#']:\n",
    "                is_mention = False\n",
    "                \n",
    "            initial_index = i\n",
    "            found_initial = True\n",
    "            mask.append(False)\n",
    "            \n",
    "        elif not found_initial:\n",
    "            mask.append(True)\n",
    "            \n",
    "            #print(decoded_t)\n",
    "    tokens_arr = np.array(tokens)\n",
    "    tokens_arr = tokens_arr[mask]\n",
    "    tokens = tokens_arr.tolist()\n",
    "            \n",
    "    return tokens, mentions, hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(l):\n",
    "    l = l.split(',')\n",
    "    t_id = l[0]\n",
    "    t_list = l[1].split('\\t')  # replace(\"\\\\n\",'').replace(\"\\\\t\",'\\t')\n",
    "    \n",
    "    return t_id, t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_strings(m_list):\n",
    "    \n",
    "    # print(m_list)\n",
    "    strings_list = []\n",
    "    \n",
    "    for m in m_list:\n",
    "        m = tok.decode(m)\n",
    "        m = m.replace(' ', '')\n",
    "        \n",
    "        strings_list.append(m)  # otherwise last string not added\n",
    "            \n",
    "    return strings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mentions_dict = {}\n",
    "current_mapping = 0\n",
    "\n",
    "def map_mentions(m_list):\n",
    "    global mentions_dict, current_mapping\n",
    "    mapped = []\n",
    "    for m in m_list:\n",
    "        if m not in mentions_dict:\n",
    "            mentions_dict[m] = current_mapping\n",
    "            current_mapping += 1\n",
    "        \n",
    "        mapped.append(mentions_dict[m])\n",
    "    \n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_dict = {}\n",
    "current_mapping_hashtag = 0\n",
    "\n",
    "def map_hashtags(m_list):\n",
    "    global hashtags_dict, current_mapping_hashtag\n",
    "    mapped = []\n",
    "    for m in m_list:\n",
    "        m = m.lower()\n",
    "        if m not in hashtags_dict:\n",
    "            hashtags_dict[m] = current_mapping_hashtag\n",
    "            current_mapping_hashtag += 1\n",
    "        \n",
    "        mapped.append(hashtags_dict[m])\n",
    "    \n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweet(index, text_tokens):\n",
    "    string = index + ',' + '\\t'.join(text_tokens)\n",
    "    result_file.write(string)\n",
    "    \n",
    "\n",
    "def save_mentions_or_hashtags(text_tokens, text, mapped, count, is_mentions=True):\n",
    "    for i in range(len(text_tokens)):\n",
    "        text_tokens[i] = '\\t'.join(text_tokens[i])\n",
    "    \n",
    "    # each mentions is separated by a \";\"\n",
    "    # each token in a mention is separated by a \"\\t\"\n",
    "    string = str(count) + ',' + ';'.join(text_tokens) + ',' + ''.join(text) + ',' + '\\t'.join(map(str, mapped)) + '\\n'\n",
    "    \n",
    "    if is_mentions:\n",
    "        mentions_file.write(string)\n",
    "    else:\n",
    "        hashtags_file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_to_int = lambda x: int(x)\n",
    "f_int = lambda x: list(map(f_to_int, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TokenizerWrapper and the dictionary to map mentions to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TokenizerWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open output files and wirte headers (column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = open(RESULT_PATH, \"w+\")\n",
    "mentions_file = open(MENTIONS_PATH, \"w+\")\n",
    "hashtags_file = open(HASHTAGS_PATH, \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file.write(TWEET_ID + ',' + TWEET_TOKENS + \"\\n\")\n",
    "mentions_file.write(\"mentions_count,mentions_tokens,mentions_text,mentions_mapped\\n\")\n",
    "hashtags_file.write(\"hashtags_count,hashtags_tokens,hashtags_text,hashtags_mapped\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open files to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file = open(TWEET_TOKENS_FILE, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row:  0  - Elapsed time:  2.384185791015625e-06\n",
      "Row:  1000000  - Elapsed time:  120.48065638542175\n",
      "Row:  2000000  - Elapsed time:  239.3873484134674\n",
      "Row:  3000000  - Elapsed time:  358.2570924758911\n",
      "Row:  4000000  - Elapsed time:  477.6771969795227\n",
      "Row:  5000000  - Elapsed time:  594.0386674404144\n",
      "Row:  6000000  - Elapsed time:  714.2335116863251\n",
      "Row:  7000000  - Elapsed time:  836.6887924671173\n",
      "Row:  8000000  - Elapsed time:  958.6229357719421\n",
      "Row:  9000000  - Elapsed time:  1078.2425425052643\n",
      "Row:  10000000  - Elapsed time:  1199.7779710292816\n",
      "Row:  11000000  - Elapsed time:  1319.963332414627\n",
      "Row:  12000000  - Elapsed time:  1440.3555297851562\n",
      "Row:  13000000  - Elapsed time:  1559.6439867019653\n",
      "Row:  14000000  - Elapsed time:  1680.6134593486786\n",
      "Row:  15000000  - Elapsed time:  1801.6696860790253\n",
      "Row:  16000000  - Elapsed time:  1921.6638708114624\n",
      "Row:  17000000  - Elapsed time:  2042.315838098526\n",
      "Row:  18000000  - Elapsed time:  2162.236151456833\n",
      "Row:  19000000  - Elapsed time:  2281.49315905571\n",
      "Row:  20000000  - Elapsed time:  2402.0451967716217\n",
      "Row:  21000000  - Elapsed time:  2520.8871965408325\n",
      "Row:  22000000  - Elapsed time:  2638.433832168579\n",
      "Row:  23000000  - Elapsed time:  2758.013892173767\n",
      "Row:  24000000  - Elapsed time:  2877.48174905777\n",
      "Row:  25000000  - Elapsed time:  2997.551997423172\n",
      "Row:  26000000  - Elapsed time:  3114.500383377075\n",
      "Row:  27000000  - Elapsed time:  3229.821130514145\n",
      "Row:  28000000  - Elapsed time:  3348.0392639636993\n",
      "Row:  29000000  - Elapsed time:  3465.945027589798\n",
      "Row:  30000000  - Elapsed time:  3583.8953769207\n",
      "Row:  31000000  - Elapsed time:  3702.8219640254974\n",
      "Row:  32000000  - Elapsed time:  3820.5357480049133\n",
      "Row:  33000000  - Elapsed time:  3934.5374763011932\n",
      "Row:  34000000  - Elapsed time:  4052.3656871318817\n",
      "Row:  35000000  - Elapsed time:  4170.010512828827\n",
      "Row:  36000000  - Elapsed time:  4289.839559793472\n",
      "Row:  37000000  - Elapsed time:  4407.924623966217\n",
      "Row:  38000000  - Elapsed time:  4526.4409902095795\n",
      "Row:  39000000  - Elapsed time:  4646.023728132248\n",
      "Row:  40000000  - Elapsed time:  4764.527948379517\n",
      "Row:  41000000  - Elapsed time:  4881.984508037567\n",
      "Row:  42000000  - Elapsed time:  4998.849872112274\n",
      "Row:  43000000  - Elapsed time:  5116.141435623169\n",
      "Row:  44000000  - Elapsed time:  5232.742671966553\n",
      "Row:  45000000  - Elapsed time:  5348.448097229004\n",
      "Row:  46000000  - Elapsed time:  5463.226462125778\n",
      "Row:  47000000  - Elapsed time:  5579.398400783539\n",
      "Row:  48000000  - Elapsed time:  5695.964301586151\n",
      "Row:  49000000  - Elapsed time:  5813.5057673454285\n",
      "Row:  50000000  - Elapsed time:  5928.093781471252\n",
      "Row:  51000000  - Elapsed time:  6042.592573881149\n",
      "Row:  52000000  - Elapsed time:  6158.128961801529\n",
      "Row:  53000000  - Elapsed time:  6277.04030585289\n",
      "Row:  54000000  - Elapsed time:  6393.777200222015\n",
      "Row:  55000000  - Elapsed time:  6508.949929475784\n",
      "Row:  56000000  - Elapsed time:  6623.023053646088\n",
      "Row:  57000000  - Elapsed time:  6738.3636775016785\n",
      "Row:  58000000  - Elapsed time:  6852.639936208725\n",
      "Row:  59000000  - Elapsed time:  6969.520555973053\n",
      "Row:  60000000  - Elapsed time:  7085.459172487259\n",
      "Row:  61000000  - Elapsed time:  7204.317008972168\n",
      "Row:  62000000  - Elapsed time:  7321.470849752426\n",
      "Row:  63000000  - Elapsed time:  7437.56901216507\n",
      "Row:  64000000  - Elapsed time:  7555.954214811325\n",
      "Row:  65000000  - Elapsed time:  7673.816046237946\n",
      "Row:  66000000  - Elapsed time:  7790.1178250312805\n",
      "Row:  67000000  - Elapsed time:  7903.757158756256\n",
      "Row:  68000000  - Elapsed time:  8017.373678922653\n",
      "Row:  69000000  - Elapsed time:  8133.180601119995\n",
      "Row:  70000000  - Elapsed time:  8245.77099275589\n",
      "Row:  71000000  - Elapsed time:  8356.094340801239\n",
      "Row:  72000000  - Elapsed time:  8468.102898836136\n",
      "Row:  73000000  - Elapsed time:  8579.926269292831\n",
      "CPU times: user 2h 21min 43s, sys: 2min 5s, total: 2h 23min 49s\n",
      "Wall time: 2h 23min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~2h 30m EXECUTION\n",
    "\n",
    "# ignore header\n",
    "line = tokens_file.readline()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "finished = False\n",
    "row = 0\n",
    "\n",
    "while not finished:  # and row < N_ROWS:\n",
    "    \n",
    "    mentions_tokens = []\n",
    "    hashtags_tokens = []\n",
    "    \n",
    "    if row % 1000000 == 0:\n",
    "        elapsed_time = time.time() - start\n",
    "        print('Row: ', row, ' - Elapsed time: ', elapsed_time)\n",
    "            \n",
    "    line = str(tokens_file.readline())\n",
    "    \n",
    "    #print(line)\n",
    "    \n",
    "    if line != '':\n",
    "        \n",
    "        tweet_id, tokens_list = split_line(line)\n",
    "        \n",
    "        #if tweet_id == '130' or tweet_id == '154' or tweet_id == '161':\n",
    "            \n",
    "        #print('\\ntweet_id: ', tweet_id)\n",
    "        #print(tokens_list)\n",
    "        #decoded_tweet = tok.decode(tokens_list)\n",
    "        #print('\\n', decoded_tweet, '\\n')\n",
    "\n",
    "        # retweets contain the word RT (right after CLS, in position 1) followed\n",
    "        # by mentions and then a ':', before starting with the actual tweet text\n",
    "        if tokens_list[1] == special_tokens['RT'] and tokens_list[2] == special_tokens['@']:\n",
    "            tokens_list, mentions_tokens = get_RT_mentions(tokens_list, mentions_tokens)\n",
    "\n",
    "        # remove remaining mentions\n",
    "        tokens_list, mentions_tokens, hashtags_tokens = get_remove_mentions_hashtags(tokens_list, mentions_tokens, hashtags_tokens)\n",
    "\n",
    "        mentions_count = len(mentions_tokens)\n",
    "        mentions_strings = convert_tokens_to_strings(mentions_tokens)\n",
    "        mapped_mentions = map_mentions(mentions_strings)\n",
    "\n",
    "        hashtags_count = len(hashtags_tokens)\n",
    "        hashtags_strings = convert_tokens_to_strings(hashtags_tokens)\n",
    "        mapped_hashtags = map_hashtags(hashtags_strings)\n",
    "\n",
    "        #print('tweet tokens: ', tokens_list)\n",
    "        #print('mentions tokens: ', mentions_tokens)\n",
    "        #print('mentions text: ', mentions_strings)\n",
    "        #print('mapped_mentions: ', mapped_mentions)\n",
    "        #print('mentions count: ', mentions_count)\n",
    "        #print('decoded tweet: ', tok.decode(f_int(tokens_list)))\n",
    "\n",
    "        #print('hashtag text: ', hashtags_strings)\n",
    "        #print('mapped_hashtags: ', mapped_hashtags)\n",
    "\n",
    "        save_tweet(tweet_id, tokens_list)\n",
    "        save_mentions_or_hashtags(mentions_tokens, mentions_strings, mapped_mentions, mentions_count, is_mentions=True)\n",
    "        save_mentions_or_hashtags(hashtags_tokens, hashtags_strings, mapped_hashtags, hashtags_count, is_mentions=False)\n",
    "\n",
    "    else:\n",
    "        finished = True\n",
    "\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file.close()\n",
    "\n",
    "result_file.close()\n",
    "mentions_file.close()\n",
    "hashtags_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save mapping dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6890195"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mentions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_mentions_mapping = json.dumps(mentions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_tokens/mentions/mentions_mapping.json', 'w+') as f:\n",
    "    f.write(json_mentions_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1872374"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashtags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_hashtags_mapping = json.dumps(hashtags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_tokens/hashtags/hashtags_mapping.json', 'w+') as f:\n",
    "    f.write(json_hashtags_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 ms, sys: 5.93 ms, total: 15.9 ms\n",
      "Wall time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(RESULT_PATH,\n",
    "                    #names=[TWEET_ID],\n",
    "                    nrows=1000,\n",
    "                    header=0,\n",
    "                    index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_features_text_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_features_tweet_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101\\t6417\\t3410\\t3398\\t3184\\t1909\\t56910\\t1683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101\\t62342\\t10858\\t54439\\t19571\\t22480\\t7831\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101\\t58955\\t10898\\t103305\\t1901\\t16181\\t7168\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101\\t2435\\t5656\\t2594\\t8279\\t8623\\t1925\\t64126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>101\\t50133\\t13028\\t18926\\t10142\\t10911\\t10142\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>101\\t42451\\t10114\\t10741\\t64312\\t10551\\t37655\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>101\\t220\\t11839\\t41541\\t10105\\t13702\\t10108\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>101\\t147\\t100\\t62691\\t35885\\t27830\\t14131\\t101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>101\\t9599\\t40311\\t119\\t14120\\t131\\t120\\t120\\t1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet_features_text_tokens\n",
       "tweet_features_tweet_id                                                   \n",
       "0                        101\\t6417\\t3410\\t3398\\t3184\\t1909\\t56910\\t1683...\n",
       "1                        101\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t12...\n",
       "2                        101\\t62342\\t10858\\t54439\\t19571\\t22480\\t7831\\t...\n",
       "3                        101\\t58955\\t10898\\t103305\\t1901\\t16181\\t7168\\t...\n",
       "4                        101\\t2435\\t5656\\t2594\\t8279\\t8623\\t1925\\t64126...\n",
       "...                                                                    ...\n",
       "995                      101\\t50133\\t13028\\t18926\\t10142\\t10911\\t10142\\...\n",
       "996                      101\\t42451\\t10114\\t10741\\t64312\\t10551\\t37655\\...\n",
       "997                      101\\t220\\t11839\\t41541\\t10105\\t13702\\t10108\\t1...\n",
       "998                      101\\t147\\t100\\t62691\\t35885\\t27830\\t14131\\t101...\n",
       "999                      101\\t9599\\t40311\\t119\\t14120\\t131\\t120\\t120\\t1...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 1.72 ms, total: 11.8 ms\n",
      "Wall time: 9.29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(MENTIONS_PATH,\n",
    "                    #names=[TWEET_ID],\n",
    "                    nrows=1000,\n",
    "                    header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mentions_count</th>\n",
       "      <th>mentions_tokens</th>\n",
       "      <th>mentions_text</th>\n",
       "      <th>mentions_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>137\\t126\\t77484</td>\n",
       "      <td>@5pm</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>137\\t10879\\t31510\\t10920\\t10138\\t168</td>\n",
       "      <td>@kireidesu_</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>137\\t33003\\t54609\\t19334\\t10123\\t26960;137\\t70...</td>\n",
       "      <td>@teenspiritjk@BTS_twt</td>\n",
       "      <td>536\\t144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mentions_count                                    mentions_tokens  \\\n",
       "0                 0                                                NaN   \n",
       "1                 0                                                NaN   \n",
       "2                 0                                                NaN   \n",
       "3                 0                                                NaN   \n",
       "4                 0                                                NaN   \n",
       "..              ...                                                ...   \n",
       "995               0                                                NaN   \n",
       "996               1                                    137\\t126\\t77484   \n",
       "997               0                                                NaN   \n",
       "998               1               137\\t10879\\t31510\\t10920\\t10138\\t168   \n",
       "999               2  137\\t33003\\t54609\\t19334\\t10123\\t26960;137\\t70...   \n",
       "\n",
       "             mentions_text mentions_mapped  \n",
       "0                      NaN             NaN  \n",
       "1                      NaN             NaN  \n",
       "2                      NaN             NaN  \n",
       "3                      NaN             NaN  \n",
       "4                      NaN             NaN  \n",
       "..                     ...             ...  \n",
       "995                    NaN             NaN  \n",
       "996                   @5pm             534  \n",
       "997                    NaN             NaN  \n",
       "998            @kireidesu_             535  \n",
       "999  @teenspiritjk@BTS_twt        536\\t144  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 7.8 ms, total: 7.8 ms\n",
      "Wall time: 85.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(HASHTAGS_PATH,\n",
    "                    #names=[TWEET_ID],\n",
    "                    nrows=1000,\n",
    "                    header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>hashtags_tokens</th>\n",
       "      <th>hashtags_text</th>\n",
       "      <th>hashtags_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>108\\t29005\\t10230;108\\t7457</td>\n",
       "      <td>#Peing#質</td>\n",
       "      <td>0\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>108\\t9670\\t20479;108\\t147\\t69849\\t11447\\t57277...</td>\n",
       "      <td>#정국#JUNGKOOK</td>\n",
       "      <td>335\\t336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hashtags_count                                    hashtags_tokens  \\\n",
       "0                 0                                                NaN   \n",
       "1                 0                                                NaN   \n",
       "2                 0                                                NaN   \n",
       "3                 2                        108\\t29005\\t10230;108\\t7457   \n",
       "4                 0                                                NaN   \n",
       "..              ...                                                ...   \n",
       "995               0                                                NaN   \n",
       "996               0                                                NaN   \n",
       "997               0                                                NaN   \n",
       "998               0                                                NaN   \n",
       "999               2  108\\t9670\\t20479;108\\t147\\t69849\\t11447\\t57277...   \n",
       "\n",
       "    hashtags_text hashtags_mapped  \n",
       "0             NaN             NaN  \n",
       "1             NaN             NaN  \n",
       "2             NaN             NaN  \n",
       "3        #Peing#質            0\\t1  \n",
       "4             NaN             NaN  \n",
       "..            ...             ...  \n",
       "995           NaN             NaN  \n",
       "996           NaN             NaN  \n",
       "997           NaN             NaN  \n",
       "998           NaN             NaN  \n",
       "999  #정국#JUNGKOOK        335\\t336  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
