{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load unique tweet tokens from file and save only those that exceed X tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 10\n",
    "BERT_SIZE_LIMIT = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad all the tweets to match with length = max_length and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = \"tweet_tokens/tweets_text_longer_than_250.csv\"\n",
    "\n",
    "TWEET_ID = \"tweet_features_tweet_id\"\n",
    "TWEET_TOKENS = \"tweet_features_text_tokens\"\n",
    "TOKENS_FILE = \"tweet_tokens/text_tokens_reduced_num_special_chars.csv\"\n",
    "\n",
    "MAX_TWEET_ID = 73549798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = open(RESULT_PATH, \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file.write(TWEET_ID + ',' + TWEET_TOKENS + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file = open(TOKENS_FILE, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n",
      "36000000\n",
      "37000000\n",
      "38000000\n",
      "39000000\n",
      "40000000\n",
      "41000000\n",
      "42000000\n",
      "43000000\n",
      "44000000\n",
      "45000000\n",
      "46000000\n",
      "47000000\n",
      "48000000\n",
      "49000000\n",
      "50000000\n",
      "51000000\n",
      "52000000\n",
      "53000000\n",
      "54000000\n",
      "55000000\n",
      "56000000\n",
      "57000000\n",
      "58000000\n",
      "59000000\n",
      "60000000\n",
      "61000000\n",
      "62000000\n",
      "63000000\n",
      "64000000\n",
      "65000000\n",
      "66000000\n",
      "67000000\n",
      "68000000\n",
      "69000000\n",
      "70000000\n",
      "71000000\n",
      "72000000\n",
      "73000000\n",
      "CPU times: user 6min 27s, sys: 5.86 s, total: 6min 33s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ABOUT ~12 MINUTES EXECUTION ON THE WHOLE DATASET\n",
    "\n",
    "# ignore header\n",
    "tokens_file.readline()\n",
    "\n",
    "finished = False\n",
    "i = 0\n",
    "\n",
    "count = 0\n",
    "indexes = []\n",
    "lines = []\n",
    "\n",
    "while not finished:  # and i < 10000000:\n",
    "    \n",
    "    if i % 1000000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    line = str(tokens_file.readline())\n",
    "    \n",
    "    if line != '':\n",
    "        \n",
    "        #line = line.replace('b','').replace(\"'\",'').replace(\"\\\\n\",'').replace(\"\\\\t\",'\\t').replace(\"102\\\\\", \"102\").replace(\"\\\\\",'').replace('\\n','')\n",
    "        splitted_line = line.split(',')\n",
    "        splitted_tokens = splitted_line[1].split('\\t')  # take only the tokens (splitted_line[0] is the tweet id)\n",
    "            \n",
    "        if len(splitted_tokens) >= 250:\n",
    "            #print(i)\n",
    "            #indexes.append(i)\n",
    "            #lines.append(line)\n",
    "            count += 1\n",
    "        \n",
    "            result_file.write(line)\n",
    "        \n",
    "    else:\n",
    "        finished = True\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file.close()\n",
    "\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
