{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the tweet with the maximum number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../tweet_tokens/subsample/text_tokens_clean_days_1_unique.csv\"  # \"../data/training/raw_columns/tweet_features/text_tokens.csv.gz\"\n",
    "\n",
    "N_ROWS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep track of the maximum tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length = 0\n",
    "tweet_with_max_length = None\n",
    "tweet_with_max_length_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_max_length(num, tweet_text, i):\n",
    "    global max_tweet_length, tweet_with_max_length, tweet_with_max_length_id\n",
    "    if num > max_tweet_length:\n",
    "        max_tweet_length = num\n",
    "        tweet_with_max_length = tweet_text\n",
    "        tweet_with_max_length_id = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load one chunk at a time and compute the maximum tweets length \n",
    "### (i.e. max number of tokens in a tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file = open(PATH, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 s, sys: 1.06 s, total: 38.1 s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# ~10 MINUTES FOR THE ENTIRE DATASET\n",
    "# ~5  MINUTES FOR THE ENTIRE DATASET WITH REDUCED SPECIAL CHARS\n",
    "\n",
    "# ignore header\n",
    "tokens_file.readline()\n",
    "\n",
    "finished = False\n",
    "i = 0\n",
    "\n",
    "while not finished:  # and i < N_ROWS:\n",
    "    \n",
    "    line = str(tokens_file.readline())\n",
    "    \n",
    "    if line != '':\n",
    "        \n",
    "        line = line.split('\\t')[1:]  # the first element contains the tweet_id\n",
    "        \n",
    "        update_max_length(len(line), line, i)\n",
    "        \n",
    "    else:\n",
    "        finished = True\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# ~35 MINUTES FOR THE ENTIRE DATASET\\n\\nfor chunk in pd.read_csv(PATH,\\n                        chunksize=CHUNKSIZE,\\n                        names=[COLUMN_NAME],\\n                        #compression=\\'gzip\\',\\n                        #nrows=N_ROWS,\\n                        header=0,\\n                        index_col=0):\\n    #print(chunk)\\n    #print(type(chunk))\\n    \\n    # Convert string into a list of strings\\n    chunk = chunk[COLUMN_NAME].str.split(\"\\t\").apply(lambda x: [int(i) for i in x])\\n    \\n    # Get the maximum tweet length in the current chunk (i.e. max number of tokens in a tweet)\\n    max_length_curr_chunk = chunk.map(lambda x: len(x)).max()\\n    #print(\"curr chunk max length: \" + str(max_length_curr_chunk))\\n    \\n    update_max_length(max_length_curr_chunk)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# ~35 MINUTES FOR THE ENTIRE DATASET\n",
    "\n",
    "for chunk in pd.read_csv(PATH,\n",
    "                        chunksize=CHUNKSIZE,\n",
    "                        names=[COLUMN_NAME],\n",
    "                        #compression='gzip',\n",
    "                        #nrows=N_ROWS,\n",
    "                        header=0,\n",
    "                        index_col=0):\n",
    "    #print(chunk)\n",
    "    #print(type(chunk))\n",
    "    \n",
    "    # Convert string into a list of strings\n",
    "    chunk = chunk[COLUMN_NAME].str.split(\"\\t\").apply(lambda x: [int(i) for i in x])\n",
    "    \n",
    "    # Get the maximum tweet length in the current chunk (i.e. max number of tokens in a tweet)\n",
    "    max_length_curr_chunk = chunk.map(lambda x: len(x)).max()\n",
    "    #print(\"curr chunk max length: \" + str(max_length_curr_chunk))\n",
    "    \n",
    "    update_max_length(max_length_curr_chunk)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"max_tweet_length.txt\", \"w\") as f:\n",
    "    f.write(str(max_tweet_length) + '\\n')\n",
    "    f.write(str(tweet_with_max_length) + '\\n')\n",
    "    f.write(str(tweet_with_max_length_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tweet_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['147',\n",
       " '119',\n",
       " '158',\n",
       " '119',\n",
       " '151',\n",
       " '119',\n",
       " '144',\n",
       " '119',\n",
       " '160',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '119',\n",
       " '152',\n",
       " '102\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_with_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3715349"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_with_max_length_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
