{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load unique tweet tokens from file and reduce the number of special chars\n",
    "\n",
    "### For example a string like \">>>>>>>>>>>>>>>>>>>>>>>>>>>>\" becomes \">>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Chars (represented by BERT's tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_char = '>' # \"& gt ;\"\n",
    "lt_char = '<' # \"& lt ;\"\n",
    "amp_char = '&' # \"& amp ;\"\n",
    "question_char = '?'\n",
    "esclamation_char = '!'\n",
    "point_char = '.'\n",
    "coma_char = ','\n",
    "dollar_char = '$'\n",
    "point_coma_char = ';'\n",
    "parenthesis_open_char = '('\n",
    "parenthesis_closed_char = ')'\n",
    "star_char = '*'\n",
    "slash_char = '/'\n",
    "line_char = '-'\n",
    "tilde_char = '~'\n",
    "virgolette_char = '\"'\n",
    "square_parenthesis_open_char = '['\n",
    "square_parenthesis_closed_char = ']'\n",
    "unk_char = '[UNK]'\n",
    "\n",
    "gt_token = '135'\n",
    "lt_token = '133'\n",
    "amp_token = '111'\n",
    "question_token = '136'\n",
    "esclamation_token = '106'\n",
    "point_token = '119'\n",
    "coma_token = '117'\n",
    "dollar_token = '109'\n",
    "point_coma_token = '132'\n",
    "parenthesis_open_token = '113'\n",
    "parenthesis_closed_token = '114'\n",
    "star_token = '115'\n",
    "slash_token = '120'\n",
    "line_token = '118'\n",
    "tilde_token = '198'\n",
    "virgolette_token = '107'\n",
    "square_parenthesis_open_token = '164'\n",
    "square_parenthesis_closed_token = '166'\n",
    "unk_token = '100'\n",
    "others_tokens = ['11733', '12022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_list = [\n",
    "    coma_token,\n",
    "    dollar_token,\n",
    "    point_coma_token,\n",
    "    parenthesis_open_token,\n",
    "    parenthesis_closed_token,\n",
    "    star_token,\n",
    "    slash_token\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_ID = \"tweet_features_tweet_id\"\n",
    "TWEET_TOKENS = \"tweet_features_text_tokens\"\n",
    "\n",
    "TWEET_TOKENS_FILE = \"tweet_tokens/text_tokens_reduced_num_special_chars.csv\"\n",
    "\n",
    "RESULT_PATH = \"tweet_tokens/text_tokens_reduced_num_special_chars_2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(l):\n",
    "    l = l.split(',')\n",
    "    t_id = l[0]\n",
    "    t_list = l[1].split('\\t')  # replace(\"\\\\n\",'').replace(\"\\\\t\",'\\t')\n",
    "    \n",
    "    return t_id, t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_to_int = lambda x: int(x)\n",
    "f_int = lambda x: list(map(f_to_int, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the header (columns names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = open(RESULT_PATH, \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file.write(TWEET_ID + ',' + TWEET_TOKENS + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open files to be read and perform the substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file = open(TWEET_TOKENS_FILE, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweet(index, text):\n",
    "    string = str(index) + ',' + '\\t'.join(text)\n",
    "    result_file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n",
      "36000000\n",
      "37000000\n",
      "38000000\n",
      "39000000\n",
      "40000000\n",
      "41000000\n",
      "42000000\n",
      "43000000\n",
      "44000000\n",
      "45000000\n",
      "46000000\n",
      "47000000\n",
      "48000000\n",
      "49000000\n",
      "50000000\n",
      "51000000\n",
      "52000000\n",
      "53000000\n",
      "54000000\n",
      "55000000\n",
      "56000000\n",
      "57000000\n",
      "58000000\n",
      "59000000\n",
      "60000000\n",
      "61000000\n",
      "62000000\n",
      "63000000\n",
      "64000000\n",
      "65000000\n",
      "66000000\n",
      "67000000\n",
      "68000000\n",
      "69000000\n",
      "70000000\n",
      "71000000\n",
      "72000000\n",
      "73000000\n",
      "CPU times: user 34min 10s, sys: 32.6 s, total: 34min 42s\n",
      "Wall time: 34min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~30 MINUTES EXECUTION ON THE WHOLE DATASET\n",
    "\n",
    "# ignore header\n",
    "tokens_file.readline()\n",
    "\n",
    "finished = False\n",
    "row = 0\n",
    "\n",
    "while not finished:  # and row < 10:\n",
    "    \n",
    "    if row % 1000000 == 0:\n",
    "        print(row)\n",
    "            \n",
    "    line = str(tokens_file.readline())\n",
    "\n",
    "    if line != '':\n",
    "\n",
    "        tweet_id, tokens_list = split_line(line)\n",
    "\n",
    "        #print('\\ntweet_id: ', tweet_id)\n",
    "        #print(tokens_list)\n",
    "\n",
    "        for special_token in special_tokens_list:\n",
    "\n",
    "            if special_token in tokens_list:\n",
    "\n",
    "                #print('special token: ' + special_token)\n",
    "                count = 0\n",
    "                index = 0\n",
    "                old_token = '101'\n",
    "\n",
    "                for token in tokens_list:\n",
    "                    if token != old_token and count > 1:\n",
    "                        #print('index: ' + str(index))\n",
    "                        #print('count: ' + str(count))\n",
    "                        #print('current token: ' + token)\n",
    "                        #print('old token: ' + old_token)\n",
    "                        tokens_list[index:index+count-1] = [special_token]\n",
    "                        count = 0\n",
    "                    elif token == special_token:\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        index += 1\n",
    "                        count = 0\n",
    "\n",
    "                    old_token = token\n",
    "\n",
    "                #print(line)\n",
    "                #print(tokens_list)\n",
    "                #time.sleep(1)\n",
    "\n",
    "        save_tweet(row, tokens_list)\n",
    "\n",
    "    else:\n",
    "        finished = True\n",
    "\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file.close()\n",
    "\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
